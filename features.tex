\PassOptionsToPackage{usenames}{color}
\documentclass[11pt,letterpaper]{article}

\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
%\usepackage{style/acl2012}
\usepackage[linkcolor=blue]{hyperref}
\usepackage[round]{natbib}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

%\usepackage{mathptmx}	% txfonts
\usepackage{fourier}
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}





% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as,assert},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0.5cm,
  belowskip=0.5cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\nasmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{NA}}_{\textsc{S}}}}}}
\newcommand{\bomarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{B}}_{\textsc{O}}}}}}
\newcommand{\jbmarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{J}}_{\textsc{B}}}}}}
\newcommand{\dbmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{D}}_{\textsc{B}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\nas}[1]{\arkcomment{\nasmarker}{#1}{red}}
\newcommand{\bo}[1]{\arkcomment{\bomarker}{#1}{blue}}
\newcommand{\jb}[1]{\arkcomment{\jbmarker}{#1}{orange}}
\newcommand{\db}[1]{\arkcomment{\dbmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{part}{\S\S#2#1#3}{and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{chapter}{\S\S#2#1#3}{and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{section}{\S\S#2#1#3}{and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subsection}{\S\S#2#1#3}{and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subsubsection}{\S\S#2#1#3}{and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
% for \label[appsec]{...}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}

\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
% \newcommand{\fref}[1]{figure~\ref{#1}}
% \newcommand{\ffref}[2]{figures~\ref{#1} and~\ref{#2}}
% \newcommand{\Fref}[1]{Figure~\ref{#1}}
% \newcommand{\FFref}[2]{Figures~\ref{#1} and~\ref{#2}}
% \newcommand{\tref}[1]{table~\ref{#1}}
% \newcommand{\ttref}[2]{tables~\ref{#1} and~\ref{#2}}
% \newcommand{\Tref}[1]{Table~\ref{#1}}
% \newcommand{\aref}[1]{algorithm~\ref{#1}}
% \newcommand{\Aref}[1]{Algorithm~\ref{#1}}
\newcommand{\fnref}[1]{\autoref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes
%\newcommand{\eref}[1]{\eqref{#1}}

% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
\newcommand{\draftnotice}[1]{#1} % for the draft version only
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}

\title{\draftnotice{{\it\small WORKING DRAFT}\\[5pt] }Features in Models of Language:\\ Parametrization Meets Implementation\thanks{Sources for this document are available at \textsmaller{\url{https://github.com/nschneid/features-note}}.}}

\author{Nathan Schneider\\ \textsmaller{\texttt{\href{mailto:nschneid@cs.cmu.edu}{nschneid@cs.cmu.edu}}}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\noindent Feature-rich models are ubiquitous in NLP. 
This note discusses concepts, terminology, algorithms, and implementation details pertaining to features in discrete statistical models.
\end{abstract}

We begin in {\bf \fullref{sec:prelim}} with a mathematical overview of the sorts of models 
that will be considered here.
New terminology (especially: \textbf{percept} and \textbf{cross-product parametrization})
and notation are explained.

The next two sections address {\em feature design}.
{\bf \fullref{sec:param}} elaborates on several mathematical issues of parametrization, 
such as the treatment of features over categorical inputs.
{\bf \fullref{sec:extraction}} offers an applied perspective on the development of feature functions---typically, 
this combines description of abstract templates with data-driven generation and selection of specific features.

Given a feature function, {\bf \fullref{sec:learning}} concerns {\em weight learning}, 
focusing on supervised training. {\bf \fullref{sec:interp}} discusses the insights that can be gained 
from examining learned model weights. 

To make the ideas in this document more concrete, 
a Python reference implementation of feature extraction and weight learning
appears in {\bf \cref{ex}}.

\section{Preliminaries}\label{sec:prelim}

We will consider feature-based models such as logistic regression 
that have discrete categorical outputs (classes, responses), which we will call {\bf labels}. 
The set of possible labels, denoted $\mathcal{Y}$, is assumed to be known and the same for all inputs.\footnote{In some cases, which we do not consider here, the space of possible outputs may depend on the input, so it is $\mathcal{Y}(x)$. 
For example, in structured problems like tagging and parsing, the space of possible predictions depends on the length of the input.}
Without loss of generality, we arbitrarily index the labels: $\mathcal{Y}=\{\mathrm{Y}^1,\mathrm{Y}^2,\ldots,\mathrm{Y}^{Y}\}$.

Given an input $x \in \mathcal{X}$, our decoding objective is 
to choose the label $y \in \mathcal{Y}$ that best fits the input. 
The function that does this, $h(x)$, is called a \textbf{classifier}.
We often operationalize $h(x)$ as maximizing a real-valued scoring function $g_\params(x,y)$, 
where $\params$ represents the model parameters: 

\begin{equation}\label{eq:classifier}
\hat{y} = h(x) = \argmax_{y\in\mathcal{Y}} g_\params(x,y)
\end{equation}

In NLP, $x$ is typically a linguistic unit---such as a word or document---with no inherent mathematical structure. 
A {\bf feature function} identifies characteristics of the input that are relevant to the mathematical model, 
and links them to the possible labels. 
For example, if $x$ is a document, then we may want to encode it as a bag of words 
in order to model similarity between documents as the number of words shared in common.

\begin{table*}\small
\begin{tabular}{ll}
$x \in \mathcal{X}$ & input (observed) \\
$y \in \mathcal{Y}$ & label (hidden; a.k.a. output, response) \\
$Y = |\mathcal{Y}|$ & number of possible labels ($\mathcal{Y}$ is discrete, finite, and fixed across instances) \\
$h: \mathcal{X}\rightarrow\mathcal{Y}$ & classifier: predicts a label given the input \\
$g_\params: \mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}$ & scoring function for an input--label pair \\
$\boldsymbol{\theta} \in \mathbb{R}^d$ & model parameters (a.k.a. weights, coefficients) \\
$\mathbf{f}: \mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}^d$ & feature function \\
$\boldsymbol{\phi}: \mathcal{X}\rightarrow\mathbb{R}^c$ & percept function (typically, $c = d/Y$) \\
\end{tabular}
\caption{Summary of notation.}
\label{tbl:notation}
\end{table*}


Our feature function, $\mathbf{f}(x,y)$, is a vector-valued mapping between 
characteristics of the input and characteristics of the output.
We assume that $\mathbf{f}(x,y)$ returns a real vector with $d$ components.

The scoring function can then depend on the sum of the components of the vector returned by $\mathbf{f}(x,y)$ weighted by corresponding {\bf parameters}.
%\footnote{The parameters in a model are sometimes called its {\bf coefficients} or {\bf weights}. 
%We reserve the term ``weight'' for the value and ``parameter'' for the variable.}
The (real) parameters weight the features to establish the goodness-of-fit between pairs of input-output characteristics.  
Here is a {\bf linear} scoring function, 
given parameters $\boldsymbol{\theta}\in\mathbb{R}^d$ such that $\theta_j$ is associated with the $j$th component of the feature vector:
\begin{equation}\label{eq:linear}
g_{\boldsymbol{\theta}}(x,y) = \sum_{j=1}^d \theta_j f_j(x,y)
= \boldsymbol{\theta}^{\top} \mathbf{f}(x,y)
\end{equation}
And here is a {\bf log-linear} scoring function, which composes the linear scoring function within the 
{\bf softmax} function (to scale it between 0 and 1), which is in turn composed within a logarithm:\footnote{The denominator (sum over all labels) is called the {\bf partition function}, sometimes denoted $Z_{\boldsymbol{\theta}}(x)$. It normalizes the scores to be a probability distribution (nonnegative and summing to 1).}
\begin{equation}\label{eq:softmax}
g_{\boldsymbol{\theta}}(x,y) = \log\frac{\exp{\sum_{j=1}^d \theta_j f_j(x,y)}}{\sum_{y'\in\mathcal{Y}}\exp{\sum_{j=1}^d \theta_j f_j(x,y')}}
= \boldsymbol{\theta}^{\top} \mathbf{f}(x,y) - \log{\sum_{y'\in\mathcal{Y}}\exp{\boldsymbol{\theta}^{\top} \mathbf{f}(x,y')}}
\end{equation}

Observe that the feature function $\mathbf{f}(x,y)$ depends on both the input and the label.
It is often useful to distinguish a {\bf percept}\footnote{This is a nonstandard term; we are introducing 
it because the term ``feature'' is overloaded, sometimes referring to the raw characteristic of the input (what we mean by ``percept''),
and other times to the variable associated with a parameter, 
which depends on both the input and the output.\nss{manning \& schuetze p. 591 mentions this terminology issue}} function, 
$\boldsymbol{\phi}(x)$---a percept being a characteristic of the input only.

Most commonly, there is one parameter for every pairing of percept with a label.\footnote{We assume 
for now that there is only one characteristic of each {\em output}, the identity of the label. \Cref{sec:richer} relaxes this assumption.}
Formally, the multiclass feature function is then:
%\nss{I am going for vector concatenation; there may be a better notation for this}
\begin{equation}\label{eq:crossprod}
\mathbf{f}(x,y=\mathrm{Y}^k) = [\overbrace{\underbrace{0, \ldots, 0\vphantom{\phi}}_{c(k-1)},\ \underbrace{\phi_1(x), \ldots, \phi_c(x)}_{c\vphantom{(}},\ \underbrace{0, \ldots, 0\vphantom{\phi}}_{c(Y-k)}}^d]^{\top}
\end{equation}
where $\boldsymbol{\phi}(x)$ describes every input with $c$ real-valued\footnote{For discrete data, these are often binary, though the models we describe here do not depend on that. A document could be featurized with relative frequencies of its words, for example.} 
characteristics (the values of these characteristics are the same no matter what label is considered), 
and $d = |\boldsymbol{\theta}| = c \times Y$.
In other words, $\left(\mathbf{f}(x,y=\mathrm{Y}^k)\right)_{c(k-1)+i} = \phi_i(x)$ for all $k\in\{1,2,\ldots,Y\}$.
We term this parametrization of the feature function $\mathbf{f}(x,y)$ as the {\bf cross-product}
parametrization, as there is one parameter for every pair of input--output characteristics.\footnote{The cross-product parameterization is the most common. 
In fact, many authors define multiclass logistic regression in terms of this parameterization; e.g., \citet{hastie-09}. 
These and other authors use the term ``feature'' to refer to what we call a ``percept.''
Some authors prefer a $Y \times c$ matrix formulation of the parameters, expressing the linear score as $\Theta \boldsymbol{\phi}(x)$ 
where $\Theta_{k,j}=\theta_{c(k-1)+j}$ in the vector formulation.}

For example: Suppose that our input $x$ is a document, and the label space $\mathcal{Y}$ consists of genre categories ({\sc article}, {\sc transcript}, {\sc recipe}, etc.).
In our terminology, one {\em percept} might be ``contains the word \textit{pie}''; the corresponding {\em features} 
would be ``{\sc article} contains the word \textit{pie}'', ``{\sc transcript} contains the word \textit{pie}'', ``{\sc recipe} contains the word \textit{pie}'', 
and so forth. When the {\sc article} label is being scored, only the features for the {\sc article} label are active,\footnote{i.e., have a nonzero value} 
and likewise for the other categories. With just these features, the model (assuming good parameters) will score 
a document with the word \textit{pie} as best matching the {\sc recipe} label, because the 
``{\sc recipe} contains the word \textit{pie}'' will have a higher weight than the other \textit{pie} features.
The model might include other percepts---other words, bigrams (``contains the bigram \textit{apple pie}''), 
metadata (``author = Martha Stewart''), or any other characteristics of the document deemed relevant to the task.
The cross-product parametrization implies that a feature is instantiated for every percept paired with every label.


Now we can define {\bf multinomial (or multiclass) logistic regression} model 
as a model over a fixed set of categorical outputs with the log-linear scoring function and the cross-product parametrization.
To simplify notation, we will let $\boldsymbol{\theta}^y$ denote the one of the $Y$ sections of $\boldsymbol{\theta}$ 
corresponding to the label $y$. 
%so $\left(\mathbf{g}_{\boldsymbol{\theta}}(x,y)\right)_j = \boldsymbol{\theta}^y_j \phi_j(x) = \boldsymbol{\theta}^{y\top} \boldsymbol{\phi}(x)$.
Then, combining equations \cref{eq:classifier,eq:softmax,eq:crossprod}, 
the classification rule is
\begin{equation}\label{eq:logistic}
\hat{y} = \max_{y\in\mathcal{Y}} g_\params(x,y) = \max_{y\in\mathcal{Y}} \boldsymbol{\theta}^{y\top} \boldsymbol{\phi}(x) - \log{\sum_{y'\in\mathcal{Y}}{\exp{\boldsymbol{\theta}^{y'\top} \boldsymbol{\phi}(x)}}}
\end{equation} 
A {\bf log-linear}, or {\bf max(imum) ent(ropy)}, model is more general---it implies the log-linear scoring function 
but does not depend on the space of outputs being fixed or require the cross-product parametrization.

\nss{is it necessary to discuss generative/discriminative/joint/conditional?}
\\

\noindent To summarize: in our terminology,
\begin{itemize}
  \item {\bf Labels} are the classes/outputs/responses. In classification, one label $y$ is predicted for each input $x$; 
  we assume the set of possible labels is fixed in advance.
  \item A {\bf percept} is a characteristic of the input. The percept function $\boldsymbol{\phi}(x)$ maps an input to a vector of percepts.
  It is typically designed manually with knowledge of the task/domain.
  \item A {\bf feature} is a real-valued variable linking a percept to a label. The feature function $\mathbf{f}(x,y)$ maps an input and a label to a vector of features.
  \item A {\bf parameter} is a real-valued variable in one-to-one correspondence with a feature. Parameters $\boldsymbol{\theta}$ are typically learned from data.
  \item The {\bf parametrization} of a model is the correspondence between percepts and features/parameters. 
  Most commonly (and always in logistic regression), the {\bf cross-product} parametrization is used, in which every percept is linked to each label, one label per feature.
  \item The {\bf scoring function} $g_{\boldsymbol{\theta}}(x,y)$ assigns a real score to a given input and label. 
  The simplest is {\bf linear} (the dot product of the parameter vector with the feature vector).
  The {\bf log-linear} scoring function is useful for probabilistic models, and subsumes logistic regression.
  In classification, the highest-scoring label for the input wins. 
\end{itemize}

\section{Parametrization}\label{sec:param}

\subsection{Bias features}\label{sec:bias}

Because the overall frequency of the labels in data may not be uniform, 
it is customary to account for this base frequency with a {\bf bias feature} 
for each label. The bias feature fires (i.e., has value 1) whenever 
the label is being scored; the weight that is learned will be indicative of 
the label's frequency.\footnote{This is equivalent to an intercept term in the scoring function.} 
Under the cross-product parametrization, this amounts to a percept that is 1 for all inputs.
Bias parameters are sometimes exempted from regularization (see \cref{sec:reg}).

\subsection{Background labels and identifiability}\label{sec:bg}

Imagine a linear binary classifier whose only percept is a real value. 
This classifier can be represented with only one feature/parameter, a threshold\footnote{In higher dimensions, the geometric boundary between classes is called a {\bf separating hyperplane}.}
below which the input is assigned to one class and above which it is assigned to the other.
One {\em could} model this decision with two features/parameters, one that fires when scoring the positive class 
and one that fires for the negative class---but this is redundant because the negative-class feature value
is a function of the positive-class feature value (they are mutually exclusive).
The model is thus said to be {\bf non-identifiable}: given infinite training data, there would be multiple 
weight vectors that could be learned while yielding identical predictions.
More generally, with $Y$ mutually exclusive labels and percept functions in $\mathbb{R}^c$, a \mbox{(log-)linear} classifier 
never requires more than $(Y-1) \times c$ features. 
Implicitly, one of the labels---call this the {\bf background label}---can be assigned a linear score of 0 for all inputs, 
and the non-background labels are scored relative to 0 (a positive score for an input means it outranks the background label; 
a negative score means it is a worse fit). 
Designating the background label $\mathrm{Y}^{\textsc{bg}}$, the linear classifier can then be rewritten as
\begin{equation}\label{eq:classifier-bg}
\hat{y} = h(x) = \argmax_{y\in\mathcal{Y}} \left\{\begin{array}{cl}
0 & \text{if }y=\mathrm{Y}^{\textsc{bg}} \\
g_\params(x,y) & \text{o.w.}\end{array}\right\}
\end{equation}
With the log-linear scoring function, it is the dot product between the parameters and the features that is 0 for the background label.

In practice, keeping parameters for all $Y$ labels (not making one of them a background label) 
should not affect predictive performance, and may make the model more interpretable (\cref{sec:interp}).
But designating a background label gives savings in memory (smaller parameter vector) and runtime (one less class to score for each input).

\subsection{Categorical characteristics}

Models for discrete data commonly represent categorical characteristics of the input.
A binary characteristic is handled with a binary-valued percept; it is redundant to have 
nonzero parameters for both the ``false'' and ``true'' states, as these would be guaranteed 
to have opposite values if learned from data (because when one fires the other is silent, and vice versa).
In general, a characteristic with a closed set of $k$ possible categorical and mutually exclusive values 
is encoded nonredundantly with $k-1$ binary percepts.\footnote{A percept taking two or more nonzero values has a scalar 
interpretation under scoring functions such as \cref{eq:linear,eq:softmax}. 
Hence the use of multiple binary percepts for categorical characteristics.}
Mathematically, there is no harm in representing all $k$ categories, 
and depending on the semantics of the characteristic, it might make the model more interpretable.
But when a characteristic has an obvious {\bf default} category (such as ``not applicable''),
it can safely be left out of the percept vector---much like features for a background label 
can be left out of the feature vector (\cref{sec:bg})---with slight computational savings.

\subsection{Scalar characteristics}

\nss{binning and thresholding for integer- or real-valued features}

\subsection{Scalar labels}

\nss{discrete set of integer- or real-valued labels, ordinal or with meaningful distances}

\subsection{Conjunctions and disjunctions}\label{sec:complex}

It is quite common to combine characteristics of the input to form a more specific percept.
For example, consider a sentiment classification task where the words of the input text have been 
tagged with a part of speech (POS). In addition to percepts for words (e.g., \textit{nice}) 
and for POS tags (e.g., \textsc{Adjective})---call these \textbf{atomic} percepts---these may be 
\textbf{conjoined} to form word-and-tag percepts (e.g., a percept that fires if and only if the input contains 
the word \textit{nice} tagged as an \textsc{Adjective}; or another that fires where the input contains 
the word \textit{nice} following an \textsc{Adverb}; and so forth).
These \textbf{complex} percepts can be more informative than the atomic percepts, 
perhaps because they are less ambiguous (\textit{nice} as adjective vs.~name of a city in France).
But conjoined percepts tend to be more sparse (less frequent in data), and thus their parameters might be harder to learn, 
so in a conditional model it is usually advisable to include both atomic and complex percepts, 
even if some are implied by others.

Another kind of complex percept that may be used is a \textbf{disjunction}: it is active if 
{\em any} of its disjuncts are active. For example, if a word is known to have spelling variants, 
like \textit{favor} and \textit{favour}, a feature that abstracts away from the spelling 
may make the model more robust to different input sources (say, American vs.~British).
Disjunctions can be interpreted as \textbf{parameter tying}---i.e., instead of separate parameters 
to be learned for \textit{favor} and \textit{favour}, these are ``tied'' together.
\nss{anything else to be said about parameter tying? do some people call it ``sharing''?}

Mathematically, complex percepts are treated just like atomic percepts, 
but the distinction can be exploited in implementation.
Feature induction (\cref{sec:induction}) seeks to discover conjunctions or disjunctions of 
existing features/percepts that improve the discriminating power of the model.


\subsection{Richer features\nss{better name?}}\label{sec:richer}

Thus far, we have assumed that features link input characteristics (percepts) 
to a single label. But in general, a feature may be any real-valued function of the input and label.
Here are two examples:

\begin{itemize}
\item \textbf{Features making sophisticated use of labels.} 
When there is a known grouping or hierarchy of labels, a feature may combine a percept 
with a disjunction of related labels. This might help the model to learn better generalizations, 
especially if some labels are much rarer than others.

\item \textbf{Features making sophisticated use of the input-and-label combination.}
Consider a preposition correction model, where the input is a preposition in context 
and the labels are candidate replacement prepositions \citep[as in][]{han-10}. 
A feature template might encode the relative frequency of the label in the input context, 
as estimated from a large corpus (perhaps separate from the data on which the model is learned).
\end{itemize}

These are mathematically straightforward, but the sophisticated relationship between input and output 
is unusual and not anticipated by many implementations.

\subsection{Other topics?}

\nss{local vs.~nonlocal in structured models}

\nss{softmax trick?}

\section{Extraction}\label{sec:extraction}

This section describes how large feature functions are designed in practice
so as to harness the mathematical machinery described above.
Margin notes link to relevant portions of the reference implementation (\cref{ex}).

The process of encoding \marginpar{\small \hyperref[lst:56]{$\rightarrow$~\textsmaller{\texttt{extract()}}}}
each data point as percepts\slash features is called \textbf{extraction}.
As discussed below, the programmer often specifies abstract \textbf{templates} 
to be instantiated (filled in) in a data-driven fashion
using the same dataset on which parameter weights will be learned, 
i.e., the \textbf{training data}. (Weight learning is discussed in \cref{sec:learning}.)
When handling features, a variety of implementation techniques 
are available to make effective use of runtime and memory resources.

\subsection{Templates}

NLP models routinely contain thousands or millions of features. 
When describing the model in code or prose, we do not actually list every single one of these features; 
instead, we employ abstractions known as \textbf{templates}. 
For instance, a document classification paper might say ``features are words in the document''---meaning, 
in our terminology, that each {\em percept} encodes a word type, and is active for any input document containing that word.
The percept template can be expressed with a variable: ``word $w$ appears in the document''; 
the template instantiates one percept for every word in some vocabulary 
(typically, the vocabulary of the training data). 
Complex templates contain multiple variables: we can generalize the first conjunction example given in the previous section to 
``the input contains the word $w$ tagged as with part of speech $p$.'' 
Universal quantification ($\forall w \in$ vocabulary, $\forall p \in$ POS tagset) 
is usually assumed, subject to limitations of the data.
Formally, a percept template $\tilde{\Phi}$ is a higher-order function 
that produces percept functions: $\forall w,p,\ \tilde{\Phi}(w, p) = \lambda w_x,p_x.\ \lsem w_x=w \wedge p_x=p \rsem$.

{\bf Open-ended templates} contain some variable whose possible values are data-dependent, not predetermined. 
Natural language words/strings are a good example.
By contrast, {\bf closed templates} contain only variables with predefined sets of possible values, 
so all instantiations of a closed template can be enumerated ahead of time. 

For a given instance and template, let \textbf{cardinality} refer to the number of active instantiations of the template.
A template's cardinality is either \textbf{fixed} across instances, or is \textbf{flexible}, i.e., it may vary from instance to instance.
E.g., consider the closed template ``input contains at least $k$ words'', where $k$ ranges from 100 to 1000 in increments of 100.
Because this template performs thresholding, anywhere from 0 to 10 percepts may be active depending on the length of the input.
But for the binning template ``input contains around $k$ words (rounded to the nearest 100)'', only one percept 
will be active per input. Similarly, an open-ended template may have a known or variable number of instantiations per input: 
``the $k$th word in the input is $w$'' ($k$ ranging from 1 to 5) has fixed cardinality, whereas 
``the input contains word $w$'' has flexible cardinality.

Sometimes the template description includes a variable for the label, 
in which case it is a feature (rather than percept) template.
Indeed, a regular mapping from percepts to features, 
as in the cross-product parametrization, can be described in terms of a feature template 
whose variables are percepts and labels: 
%``percept $\phi$ and label $y$ $\forall \phi\in\Phi, y\in\mathcal{Y}$.''
$\forall \phi\in\Phi, y\in\mathcal{Y},\ \tilde{F}(\phi,y) = \lambda x,y'.\ \phi(x) \cdot \lsem y'=y \rsem$.

\nss{feature template metalanguages}

Feature extraction code often handles both nontemplatic percepts (e.g., ``input starts with a capital letter'') 
and templatic percepts (e.g., ``input starts with the character $k$'') together.

\subsection{Instantiation and recognition}\label{sec:inst}

Feature extraction code has to be used for two tasks:
\begin{itemize}
  \item \textbf{Instantiation:} \marginpar{\small \hyperref[lst:81]{$\rightarrow$~\textsmaller{\texttt{instantiate()}}}}
  Decide which percepts\slash features will be included in the feature function used by the model. 
  If there are templates (particularly open-ended ones), an initial pass is usually made 
  to determine which instantiations of the templates are needed for the training data.
  \item \textbf{Recognition:} \marginpar{\small \hyperref[lst:129]{$\rightarrow$~\textit{line 129}}}
  Given the model's feature function, enumerate the active percepts\slash features and their values.
  This is performed for each instance during training\footnote{To avoid repeating extraction on the training data 
during each iteration of learning, consider caching each instance's percepts\slash features (see \nss{TODO}).} and at test time.
\end{itemize}

%\paragraph{Unsupported percepts and features.}
Many percepts and features, especially instantiations of open-ended templates, will never be active in the training data. 
These are said to be \textbf{unsupported}.
Note that no unsupported {\em percept} will receive nonzero parameter weights during learning, 
so only percepts attested in the training data (found during instantiation) 
need to be included in the feature function (and accessed during recognition).\footnote{In the framework of \mbox{(log-)linear} 
models addressed here, the only exception can be if a special regularizer is used that might push weights for unsupported percepts away from 0. 
See \nss{TODO}.}
To put this in perspective, instantiating a word template for every vocabulary item in the English language would be vastly ineffecient!
On the other hand, unsupported {\em features} for supported percepts---percept-label pairs never seen in the training data---may be retained, 
and can be slightly beneficial to the model by helping steer the classifier away from incorrect predictions. 

%\paragraph{Cutoffs.}
Sometimes a \textbf{cutoff} is used to remove infrequent percepts\slash features as measured by the training data.
Active percepts\slash features are counted during instantiation, and then any occurring fewer than $k$ times 
are discarded from the feature function. (The usual no-unsupported-percepts rule amounts to $k=1$ for percepts, 
though this can be made stricter; a no-unsupported-features policy would apply a $k\geq 1$ cutoff to individual features.)
For sufficiently small (relative to the size of the training data) values of $k$, 
this will have little impact on accuracy but will make learning and classification more efficient. 

\nss{(more sophisticated) feature selection techniques?}

\subsection{Implementing extraction and lookup}

Extracting/accessing a large number of features can be a major performance bottleneck. 
We discuss the efficiency tradeoffs of several feature handling optimizations.

\subsubsection{Optimizations for both memory and runtime}

\begin{itemize}
\item \textbf{Sparse vs.~dense data structures.} 
\marginpar{\small \hyperref[lst:57]{$\rightarrow$~\textsmaller{\texttt{active\_percepts}}}}
The number of active percepts\slash features for an instance 
is generally much smaller than the number of parameters \nss{refer to sparsity discussion above}, 
so a sparse data structure such as a map should be used rather than a length-$d$ vector.
\marginpar{\small \hyperref[lst:102]{$\rightarrow$~\textsmaller{\texttt{weights}}}} 
On the other hand, a dense vector/array data structure should be used for the parameter weights
if most weights will be nonzero (that is, unless the model enforces sparsity).

\item \textbf{Feature indexing.}
Features should be indexed to integers so weights can be stored in an array for fast dot product computation.
\begin{itemize}
\item \textbf{Implicit feature indexing.} \marginpar{\small \hyperref[lst:66]{$\rightarrow$~\textsmaller{\texttt{feature\_index()}}}}
With the cross-product parametrization, \cref{eq:crossprod}, 
feature indices should be computed on the fly.
First number the labels, 
and map percept names to integers (by storing an explicit mapping to unique indices, or by hashing).
The weight vector can then be organized such that the feature index 
is a simple function of the percept and label indices.
\end{itemize}

\item Choose a \textbf{background label} (\cref{sec:bg}), 
\marginpar{\small \hyperref[lst:74]{$\rightarrow$~\textsmaller{\texttt{labels[-1]}}}}
and\slash or apply a percept or feature \textbf{cutoff} 
\marginpar{\small \hyperref[lst:34]{$\rightarrow$~\textsmaller{\texttt{percept\_cutoff}}}}
such as removing unsupported features (\cref{sec:inst}), 
in order to limit the dimensionality of the feature function.
It is worth experimenting with a few alternatives, because cutoffs---especially 
in combination with a background label\footnote{Consider a percept $\phi$ observed frequently in the training
data, always with label $y$. Intuitively, this percept should be a good predictor of the label $y$---but
if $y$ is designated as the background label and no unsupported features are included,
$\phi$ will not be represented at all in the model!}---may affect the model's predictive power.
\end{itemize}

\subsubsection{Optimizations for memory}

\begin{itemize}
\item \textbf{String vocabulary compression.}
Na\"{i}ve handling of strings may result in many duplicate copies in memory 
(e.g., of any word that appears in multiple instances). 
There are a few techniques to avoid this when loading instances or during extraction:
\begin{itemize}
\item \textbf{Interning.} This operation checks to see whether an identical string has been previously interned, 
and if so returns that copy to eliminate duplication.
Interning functionality is usually built in to the programming language (e.g., via an \textsmaller{\texttt{intern()}} method on strings), 
though a map can be used instead to store canonical copies of strings.
% mention that IdentityHashMap is available if all strings are interned?
\item \textbf{Explicit indexing.} \marginpar{\small \hyperref[lst:41]{$\rightarrow$~\textsmaller{\texttt{index\_string()}}}}
Assign a unique integer to each string that is read or produced 
and use the integers (rather than strings) in the instance's features. 
This is the most straightforward approach to indexing.
A separate mapping between strings and integers will have to be maintained. 
\item \textbf{Hashing.} Strings can instead be hashed to obtain an index, 
though the index is not guaranteed to be unique.
\end{itemize}

\item \textbf{Bundling.} \marginpar{\small \hyperref[lst:45]{$\rightarrow$~\textit{line 45}}}
Storing each complex percept in a lightweight tuple-like data structure rather than as a concatenated string 
allows atomic parts of the percept to be reused. 
(This may also improve runtime if string concatenation is expensive.)
\end{itemize}

\subsubsection{Optimizations for extraction runtime}

\begin{itemize}
  \item \textbf{Indexing data structures.} For fast lookup with explicit indexing of strings or tuples, 
use a trie or a map with an efficient hash function.

  \item \textbf{Percept/feature hashing.} An extreme form of percept or feature indexing is to use a hash function to compute the index 
(without having to consult a data structure).\footnote{See \citep{weinberger-09} and \url{http://en.wikipedia.org/wiki/Feature_hashing}.}
With a fast hash function, this reduces runtime overhead, 
and if the names of the percepts\slash features are discarded following hashing, it saves memory as well.
The downside is that hash values are not guaranteed to be unique; 
thus, there is a chance that multiple percepts\slash features will 
be mapped to the same parameter, though occasional collisions should have minimal impact in practice. 

  \item \nss{parallelization?}
\end{itemize}

\subsubsection{Optimizations for scoring runtime}

\begin{itemize}
\item \textbf{With only binary features.} If all percepts\slash features in the model are binary, 
then the dot product score is simply the sum of weights of active features: no multiplication is required.
\end{itemize}

% - alternative suggested by Sam: use locality-sensitive hashing to project to a low-dimensional binary feature space. 
%   then, instead of a dot product, compute a Hamming distance. 
%   in learning, the training instances are projected once, and the weight vector is re-projected each time 
%   it is updated (so probably not ideal for online learning algorithms). 
%   see \citep{?} for details.


\section{Learning}\label{sec:learning}

This document is not really intended as a tutorial on machine learning modeling or learning per se. 
However, we will give an overview of some important approaches to learning feature-based models; 
a much more thorough treatment appears in \citet{smith}. 

Let the labeled \textbf{training data} $\mathcal{D}$ be a collection of $(x,y)$ pairs.
\textbf{Supervised learning} is the process of estimating model parameters from labeled data.
This is formulated generically as choosing parameters to minimize a \textbf{loss function} that scores the 
``badness'' of the training data according to the model:
\begin{equation}
\hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}} \textit{loss}(\mathcal{D}; \boldsymbol{\theta})
\end{equation}
Some modeling assumptions and loss functions make this procedure easier than others.
All of the loss functions considered below are \textbf{convex}, 
meaning the loss function does not suffer from local optima.
Convex and nonconvex optimization are entire subfields in their own right; 
we address just a couple of popular convex loss functions and optimization methods here.

\subsection{Na\"{i}ve Bayes and generative vs.~conditional models}\label{sec:naive}

The probabilistic feature-based multiclass classification model family known as \textbf{na\"{i}ve Bayes} assumes:
\begin{itemize}
  \item the cross-product parametrization is used, and
  \item percepts are statistically independent of each other conditioned on the label.
\end{itemize}

Because of the independence assumption, na\"{i}ve Bayes models decompose into two kinds of probability distributions:
the \textbf{class prior} $p(y)$, and a conditional distribution $p_i(\phi_i(x) \mid y)$ for each percept $\phi_i$.
The independence assumption can be understood as instantiating the following \textbf{generative derivation}:
first, the label $y$ is chosen stochastically from the prior distribution over labels; 
then, conditioned on that choice, the percept values are independently drawn from the conditional distributions.

The joint probability under the model is
\begin{equation}
p(x,y) = p(y)\prod_i{p_i(\phi_i(x) \mid y)}
\end{equation}
By Bayes' rule, the classifier under such a model is
\begin{eqnarray}
\hat{y} = h(x) &=& \argmax_y{p(y \mid x)} = \argmax_y{p(x,y)} \\
               &=& \argmax_y{p(y)\prod_i{p_i(\phi_i(x) \mid y)}}
\end{eqnarray}
This is a special case of a classifier using the linear scoring function, \cref{eq:linear}, 
where parameters are log probabilities: $\theta^y_i = \log{p_i(\phi_i(x) \mid y)}$ for non-bias percepts ($i>0$), and 
$\theta^y_0 = \log{p(y)}$ for the bias percept, so $g_{\boldsymbol{\theta}}(x,y)=\log{p(x,y)}$.\footnote{Because under these assumptions, 
the linear scoring function computes a log probability, the normalization in the log-linear scoring function of \cref{eq:softmax} would be redundant.}

The most popular flavor is \textbf{multinomial} na\"{i}ve Bayes, in which the 
percepts are binary (or categorical), and 
the prior and conditional distributions are all modeled as (discrete) multinomial distributions, i.e.~lookup tables 
whose probabilities must be nonnegative and sum to 1.\footnote{The generative derivation can be written as: 
$y \sim \textsl{Mult}(\boldsymbol{\theta}^{\text{prior}})$; $\forall i,~\phi_i \sim \textsl{Mult}(\boldsymbol{\theta}^y_i)$.}

Maximum likelihood estimation of a multinomial na\"{i}ve Bayes model amounts to minimizing the joint \textbf{log loss}:
\begin{eqnarray}
\hat{\boldsymbol{\theta}} &=& \argmin_{\boldsymbol{\theta}} \textit{loss}(\mathcal{D}; \boldsymbol{\theta})
=   \argmin_{\boldsymbol{\theta}}{\sum_{(x,y) \in \mathcal{D}}{-\log{p_{\boldsymbol{\theta}}(x,y)}}} \\
&=& \argmin_{\boldsymbol{\theta}}{\sum_{(x,y) \in \mathcal{D}}{-\log{\left(p_{\boldsymbol{\theta}^{\text{prior}}}(y)\prod_i{p_{\boldsymbol{\theta}^y_i}(\phi_i(x) \mid y)}\right)}}}
\end{eqnarray}
Note that the log loss is equal to the negative log likelihood, which is why minimizing loss maximizes likelihood.
Conveniently, there is a closed form solution---the multinomials are estimated simply by 
counting relative frequencies in the training data:
\begin{eqnarray}
\hat{p}(y) &=& \frac{|\{(x',y')\in\mathcal{D}: y'=y\}|}{|\mathcal{D}|} \\
\hat{p}_i(\phi_i(x) \mid y) &=& \frac{|\{(x',y')\in\mathcal{D}: y'=y~\wedge~\phi_i(x')=\phi_i(x)\}|}{|\{(x',y')\in\mathcal{D}: y'=y\}|}
\end{eqnarray}

Na\"{i}ve Bayes models are said to be \textbf{generative} because they model 
the joint distribution $p(x,y)$, as opposed to \textbf{conditional} alternatives that only model $p(y \mid x)$.
In other words, a generative model has to encode information about the plausibility of the input as well as the label, 
whereas a conditional model takes the input at face value and assigns a probability to the label based on that.
A conditional or marginal distribution can be recovered from the joint, but not vice versa.

Despite the advantages of na\"{i}ve Bayes noted above---(a)~its representation of the full joint distribution, 
and (b)~its simple and cheap supervised estimation procedure---there is a downside. 
The assumption that all percepts are conditionally independent is usually, well, incredibly {\em na\"{i}ve}. 
Notably, it is not uncommon in NLP to include \textbf{overlapping percepts\slash features} 
that incorporate common aspects of the input. (For example, the percepts ``word is \textit{Martha}'' 
and ``word contains a capital letter'' are not independent because the former implies the latter.)
With large numbers of percepts over strings, interdependencies among them may be difficult to 
characterize, so the independence assumption can be disadvantageous. 

Because conditional classification models estimate only a conditional distribution and 
do not (wrongly) assume feature independence, they usually give better classification accuracy 
given large amounts of supervision. But giving up the independence assumptions 
means we have to turn to a more sophisticated estimation procedure.

\subsection{Discriminative objectives}

In \textbf{discriminative} learning, we seek to find a model that does a good job choosing 
(\textit{discriminating}) between labels given some input. 
We do not have to worry about modeling how likely the input is a priori.
Models learned discriminatively need not even be probabilistic (though some are).

One popular discriminative loss function is the \textbf{conditional log loss}:
\begin{equation}\label{eq:logloss}
\textit{loss}_\text{cond. log}(\mathcal{D}; \boldsymbol{\theta}) = \sum_{(x,y)\in\mathcal{D}}{-\log{p_{\boldsymbol{\theta}}(y \mid x)}}
\end{equation}
The log-linear scoring function, \cref{eq:softmax}, can be interpreted as a giving a log conditional probability, 
allowing us to rewrite the above as 
\begin{equation}
\sum_{(x,y)\in\mathcal{D}}{\left(-\boldsymbol{\theta}^\top\mathbf{f}(x,y)+\log{\sum_{y'\in\mathcal{Y}}{\exp{\boldsymbol{\theta}^\top\mathbf{f}(x,y')}}}\right)}
\end{equation}
A model with this loss will learn to produce a probability distribution over possible labels given an input.
The loss's gradient with respect to parameter $j$ is
\begin{equation}\label{eq:loggrad}
\nabla_j = \sum_{(x^*,y^*)\in\mathcal{D}}-f_j(x^*,y^*)+\underbrace{\sum_{y\in\mathcal{Y}} f_j(x^*,y)\frac{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x^*,y)}}{\sum_{y'\in\mathcal{Y}}{\exp{\boldsymbol{\theta}^{\top}\mathbf{f}(x^*,y')}}}}_{\mathbb{E}_{p_{\boldsymbol{\theta}}(y = \bullet \mid x=x^*)} f_j(x^*,\bullet)}
\end{equation}
Assuming the cross-product parametrization, 
the loss's gradient with respect to the parameter associated with percept $i$ and label $y$ is
\begin{equation}\label{eq:logperceptgrad}
\nabla^y_i = \left(\sum_{(x^*,y^*=y)\in\mathcal{D}}-\phi_i(x^*)\right)+\left(\sum_{(x^*,y^*)\in\mathcal{D}}{\underbrace{\phi_i(x^*)\frac{\exp{{\boldsymbol{\theta}^y}^{\top}\boldsymbol{\phi}(x^*)}}{\sum_{y'\in\mathcal{Y}}{\exp{{\boldsymbol{\theta}^{y'}}^{\top}\boldsymbol{\phi}(x^*)}}}}_{\mathbb{E}_{p_{\boldsymbol{\theta}}(y = \bullet \mid x=x^*)} \phi_i(x^*)}}\right)
\end{equation}
In both cases, the first term consists of \textbf{empirical feature counts}, 
i.e., the sum of values for this feature over all seen instances; 
it is constant given the labeled training data. The second term consists of \textbf{expected feature counts} 
under the model, i.e., what the feature counts would be if the model were allowed to (softly) predict the labels 
for training examples using the current parameters. The objective of fitting the data, then, 
can be understood as adjusting the parameters so that the expected feature counts match the empirical feature counts.\footnote{\nss{refer to \citep{smith} for maxent interpretation}}

If the goal is to predict the single best label, the \textbf{max-margin loss} is a popular alternative:\footnote{The \textbf{cost function} assigns a nonnegative penalty to erroneous predictions during learning.
\nss{relationship to margin?} 
With $\textit{cost}(y,y')=\lsem y'\neq y\rsem$, i.e.~all errors incur a cost of 1, this is called the \textbf{hinge loss}. 
With no cost, it is the classic \textbf{perceptron} loss.
All can be considered approximations to the discontinuous \textbf{0--1 loss},
$\sum_{(x,y)\in\mathcal{D}}{\lsem \left(\argmax_{y'}{\boldsymbol{\theta}^\top\mathbf{f}(x,y')}\right)\neq y \rsem}$.\nss{right?}
A cost function can be incorporated into a log-linear learning objective as well: this is called \textbf{softmax-margin} \citep{gimpel-10}.}
\begin{equation}\label{eq:marginloss}
\textit{loss}_\text{margin}(\mathcal{D}; \boldsymbol{\theta}) = \sum_{(x,y)\in\mathcal{D}}{\left(- \boldsymbol{\theta}^\top\mathbf{f}(x,y) + \left(\max_{y'}{\boldsymbol{\theta}^\top\mathbf{f}(x,y') + \textit{cost}(y,y')}\right)\right)} 
\end{equation}
A subgradient with respect to the $j$th parameter is:\footnote{Note that this is {\em negative} 
for a feature that should have been active, but wasn't 
(because the cost-augmented prediction is incorrect under the current parameters), 
and {\em positive} for a feature that shouldn't have been active, but was under the current parameters.
(We are minimizing, so lower is better.)
Either way, the magnitude for the component of the gradient is simply the absolute value of the corresponding 
feature---unless the cost-augmented predictor is correct, in which case it is 0. 
The rationale for the cost function is to force the learner to make more errors when the score of a competitor label 
is too close to the score of the gold label (the \textbf{margin} between them is below threshold);
consequently, for such ``close calls,'' the learner will make corrections to the parameters out of an abundance of caution.}
\begin{equation}\label{eq:margingrad}
\nabla_j = \sum_{(x,y)\in\mathcal{D}} -f_j(x,y) + f_j(x,\left(\argmax_{y'}{\boldsymbol{\theta}^\top\mathbf{f}(x,y')+\textit{cost}(y,y')}\right))
\end{equation}
Again, we see the empirical feature counts in the first term, and predicted feature counts in the second term.
This is cheaper to optimize than the log loss, because it does not require computing a partition function 
(sum of scores for all labels) during learning.
Unlike a probabilistic loss, as long as the model assigns the best score to the correct label, 
the hinge loss does not care about the other labels' scores relative to one another. 

The subgradients in \cref{eq:loggrad,eq:logperceptgrad,eq:margingrad} 
are summations over the training data instances, and as such can be decomposed 
for the benefit of online methods treating one instance at a time (\cref{sec:online}).

\nss{consider explaining connections to SVMs, max-margin networks}

\subsection{Regularization}\label{sec:reg}

To prevent the learner from getting carried away in fitting the training data, 
it is common practice to use the learning objective
\begin{equation}
\hat{\boldsymbol{\theta}}, \hat{\lambda} = \argmin_{\boldsymbol{\theta}, \lambda}\ell(\mathcal{D}, \boldsymbol{\theta}, \lambda) = \argmin_{\boldsymbol{\theta}, \lambda}\textit{loss}(\mathcal{D}; \boldsymbol{\theta}) + \lambda R(\boldsymbol{\theta})
\end{equation} 
where the $\lambda R(\boldsymbol{\theta})$ term provides \textbf{regularization}.
The nonnegative function $R(\boldsymbol{\theta})$ expresses a preference for certain models over others; 
most typically, this means penalizing the model's complexity (distance from $\mathbf{0}$, according to a given metric).
The regularization trades off with the loss---parameters that better fit the training data will tend to have a higher complexity. 
A nonnegative hyperparameter, $\lambda$, controls this tradeoff. 
$\lambda$ is usually tuned in an outer loop by line search, 
and the value tried that gives the model best predictive accuracy on a held-out development set is chosen.

The most popular flavors are $L_2$ (or \textbf{ridge}) and $L_1$ (or \textbf{Lasso}) regularization:\footnote{These can also be combined in what is known as \textbf{elastic net} regularization.}
\begin{eqnarray}
R_{L_2}(\boldsymbol{\theta}) = \tfrac{1}{2} ||\boldsymbol{\theta}||^2_2 = \tfrac{1}{2} \sum_{j=1}^d \theta^2_j &&
R_{L_1}(\boldsymbol{\theta}) = ||\boldsymbol{\theta}||_1 = \sum_{j=1}^d |\theta_j|
\end{eqnarray}
Both are convex, so with a convex loss function, the full learning objective will remain convex.
$\partial R_{L_2} / \partial \theta_j = \theta_j$, 
so the $L_2$ penalty is trivial to implement.
$L_1$~regularization encourages many parameters to go to 0, but the function is not differentiable, 
and as such requires special optimization techniques.
$L_2$~regularization tends to work at least as well as $L_1$ in terms of producing models
that are accurate predictors.

Means of preventing overfitting apart from modifying the learning objective might be called \textbf{procedural regularization}. 
These include:
\begin{itemize}
  \item Choosing the number of iterations of optimization by \textbf{early stopping}, i.e., 
  monitoring predictive accuracy on held-out development data and stopping once that accuracy fails to improve.
  \item \textbf{Averaging} (or \textbf{voting} among) models of different optimization ``ages'' \citep{freund-99}.
  \item \textbf{Dropout} \citep{hinton-12,baldi-13}, a technique in which some percepts of each training instance are zeroed out 
  at random. \citet{wager-13} describe a close connection between dropout and online learning with AdaGrad (see \fnref{fn:adaptive}).
\end{itemize}

Procedural regularization techniques are most closely associated with the perceptron loss, 
which is invariant to the scale of the weight vector---the perceptron only cares that the true label gets the highest score; 
it does not care by how much---so a regularization term in the objective will have no effect with the perceptron.
Objective regularization is standard for the log loss 
and can also be used with a max-margin loss that has a nonzero cost function.\nss{right?}

\subsection{Sparsity}\label{sec:sparsity}

When talking about feature-rich models, the word \textbf{sparse} can mean one of three things:
\begin{itemize}
  \item \textbf{data sparseness}: Characterizing a dataset as ``sparse'' is a fancy way of saying there is not as much of it as we would like 
  given the complexity of what we are trying to learn.
  \item \textbf{vector sparsity}: We say that a vector (or matrix) is ``sparse'' if it has few nonzero components.
  Formally, some vector $\mathbf{v}$ is said to be sparse if $||\mathbf{v}||_0 \ll |\mathbf{v}|$.\footnote{$||\mathbf{v}||_0$, the $\ell_0$-norm, counts the number of nonzero components of $\mathbf{v}$.} 
  This can manifest itself in two ways in our models:
  \begin{itemize}
  	\item \textbf{weight sparsity}: When learning weights for a large number of parameters, it may be desirable to encourage
  many or most of them to be zero on the hypothesis that a relatively small number of parameters are actually 
  relevant to the task. 
  $L_1$ regularization (\cref{sec:reg}) is one technique for biasing the learner in favor of models where 
  $||\boldsymbol{\theta}||_0 \ll |\boldsymbol{\theta}|$.\footnote{In fact, the $\ell_1$-norm $||\boldsymbol{\theta}||_1$ can be viewed as an approximation of the $\ell_0$-norm, 
  which is more difficult to optimize.}
	  \item \textbf{instance sparsity}: With language data, it is usually the case that the percept\slash feature 
  vector for a particular data instance has very few active components relative to the dimensionality.
  This is because string vocabularies are large, resulting in many (mostly individually rare) binary features.  
  As we shall see in \cref{sec:impllearning}, exploiting instance sparsity is crucial to implementing learning efficiently.
  \end{itemize} 
\end{itemize}

\subsection{Optimization}

Having formulated a learning objective consisting of a loss function and, optionally, a regularization term, 
we can exploit numerical optimization techniques for learning a set of parameters that minimize this function.
Here we give a synopsis of a simple online learning algorithm that is widely used in NLP, 
and discuss practical considerations required for making it fast.

\nss{practical issues (e.g., fast score computation, sparse updates/averaging)}
% (kinds of optimization: online vs. (mini)batch; gradient-based; margin-based vs. probabilistic; curriculum; joint/constrained/dual decomp)

\nss{unsupported features/count thresholds; OOV handling}

\subsubsection{Online optimization: SGD}\label{sec:online}

\textbf{Online} optimization algorithms pass through the training data and incrementally update 
the weights to better fit the data.
\textbf{Stochastic gradient descent} \citep[SGD;][]{sgd} makes one or more passes---called \textbf{epochs} or \textbf{iterations}---through the data. 
At time $t \geq 1$, the learner receives an instance $(x^{(t)},y^{(t)})$ and performs an update of the form
\begin{equation}
\boldsymbol{\theta}^{(t)} \leftarrow \boldsymbol{\theta}^{(t-1)} + {\boldsymbol{\alpha}^{(t)}}^{\top} \boldsymbol{\nabla}(x^{(t)},y^{(t)};\boldsymbol{\theta}^{(t-1)})
\end{equation}
where $\boldsymbol{\nabla}(x,y; \boldsymbol{\theta})$ is the (sub)gradient under parameters $\boldsymbol{\theta}$ 
of the learning objective with respect to the instance; the counter $t$ is incremented for every instance.
$\boldsymbol{\alpha}$ is the \textbf{learning rate} or \textbf{step size}---it weights the new evidence 
from the current instance against the cumulative evidence from past timesteps.
The learning rate may be a constant scalar hyperparameter (e.g., $1$ or $.9$), may be scheduled to decrease over time, 
or may be chosen adaptively using the gradient.\footnote{\label{fn:adaptive}Adaptive techniques include DCA \citep{martins-10} 
and AdaGrad \citep{adagrad}; see also \url{http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf}.
With DCA, $\boldsymbol{\alpha}^{(t)}$ is a scalar; with AdaGrad it is a vector (i.e., each parameter has its own learning rate).}
The parameters are typically initialized to 0: $\boldsymbol{\theta}^{(0)} \leftarrow \mathbf{0}$.

Alternatives for optimizing the learning objective include 
\textbf{batch} algorithms, which compute statistics over the full training data 
before making each update to the model weights. 
L-BFGS \citep{lbfgs} is a popular batch optimizer for which there are open source implementations.
\textbf{Mini-batch} optimization is a middle ground in which several training instances 
are used for each update \citep[p.~175]{smith}.

\subsubsection{Implementing learning}\label{sec:impllearning}

Now we consider (computational) optimizations for (parameter) optimization.

\nss{implementation issues: store features in memory/disk?; sparse updates}

% runtime savings during learning
% - cache the extracted percepts for each instance so they do not have to be recomputed on each iteration





\section{Interpretation}\label{sec:interp}

\nss{caveats about interpretability}

\nss{impact measure}

\nss{standardization (divide real-valued features by variance?)}

\section{Advanced topics}

\subsection{Feature induction}\label{sec:induction}

\nss{learning conjunctive features; learning low-dimensional representations}

\subsection{Nonlinear models}

\nss{kernels; greedy search--based classifiers like decision trees?; \ldots}

\subsection{Structured prediction}

Some important unstructured classifiers vs.~structured models in NLP:

\begin{center}\small
\begin{tabular}{@{}>{\raggedright\bf}p{9em}>{\raggedright}p{13em}@{~~}>{\raggedright\arraybackslash}p{16em}@{}}
& \multicolumn{1}{c}{Unstructured} & \multicolumn{1}{c}{Sequence} \\
\hline
generative, probabilistic & multinomial na\"{i}ve Bayes (\cref{sec:naive}) & {\raggedright multinomial discrete hidden Markov model (HMM)} \\
\hline
conditional, probabilistic & log-linear classifiers (including logistic regression) & conditional random field (CRF)---globally normalized; 
maximum entropy Markov model (MEMM)---locally normalized \\
\hline
conditional, non-probabilistic & linear classifiers trained with a max-margin loss (including perceptron) & structured perceptron \citep{collins-02} (sometimes cost-augmented) \\
\hline
\end{tabular}
\end{center}

\nss{feature locality/factorization for dynamic programming; also cost function needs to factorize; 
approximate inference; hard constraints/ILPs/dual decomp; combining generative and discriminative (Berg-Kirkpatrick)}

\subsection{Multitask learning and domain adaptation}

\nss{interesting things with features: frustratingly easy, Ando \& Zhang, \ldots}

\subsection{Unsupervised and semisupervised learning}

\nss{pointers to un/semi-supervised approaches, graph-based}
% - semi-supervised learning/domain adaptation: posterior regularization; active learning; transfer learning
% - unsupervised learning: generative better?; Berg-Kirkpatrick

The framework of \textbf{posterior regularization} \citep{ganchev-10} 
allows declarative assumptions about good analyses (e.g., in part-of-speech tagging, ``every sentence should contain at least one verb'') 
to be encoded as expectation contraints in a regularizer that uses unlabeled data.
In this framework, the model features are supplemented with special features for the constraints.
Both kinds of features are required to factor locally for tractability of inference.

Globally-normalized conditional models are often more effective than generative models in supervised settings 
due to their ability to incorporate rich overlapping features, but they are difficult to learn unsupervised.
One tractable solution is \textbf{contrastive estimation}, 
which uses an approximate partition function derived from quasi-negative evidence \citep{smith-05}. 
An alternative approach to unsupervised modeling is to parametrize a structured generative model (such as an HMM) 
with locally normalized, feature-rich log-linear models for some of its component distributions. 
\citet{berg-kirkpatrick-10} offer learning techniques for such ``generative-conditional hybrid'' models 
and obtain state-of-the-art performance on several tasks.

\subsection{Structured sparsity}

\nss{group lasso, etc.}



\section*{Acknowledgments}

Thanks to the members of the \href{http://www.ark.cs.cmu.edu/}{Noah's ARK} lab at CMU 
for their valuable suggestions, discussions, and feedback.

\appendix
\section{Reference implementation}\label[appsec]{ex}

We include a Python implementation to demonstrate some of the feature handling and learning techniques 
discussed in this document.
Source code and data can be downloaded from \textsmaller{\url{https://github.com/nschneid/features-note/}}.

\subsection{Code}

%caption={Example classifier implementation in Python.},
\lstinputlisting{classifier.py}

\subsection{Exercises}

\begin{enumerate}
  \item Write a function to print the learned feature weights, 
  including human-readable feature names and the associated class label.
  Which ones appear to be the most important?
  Are any of the features redunant?
  \item Implement the impact measure given in \cref{sec:interp} 
  to see which features make the biggest difference on the test set.
  Are these the same as the features with strongest weights?
  \item Experiment with adding new features (or improving existing ones).
  How do they affect performance?
  \item Experiment with tuning hyperparameters such as the number of training epochs
  and the percept cutoff. How do those affect performance?
  \item Implement the hinge loss cost function. Is it better than the perceptron?
  \item Add a learning rate schedule such as AdaGrad. Does it lead to faster convergence?
  \item {\em (Advanced)} Add support for logistic regression (the log loss) in training and decoding. 
\end{enumerate}

\bibliographystyle{plainnat}
\bibliography{features}



\end{document}
