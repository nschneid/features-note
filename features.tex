\PassOptionsToPackage{usenames}{color}
\documentclass[11pt,letterpaper]{article}

\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
%\usepackage{style/acl2012}
\usepackage[linkcolor=blue]{hyperref}
\usepackage[round]{natbib}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

%\usepackage{mathptmx}	% txfonts
\usepackage{fourier}
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}





% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0cm,
  belowskip=0cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\nasmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{NA}}_{\textsc{S}}}}}}
\newcommand{\bomarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{B}}_{\textsc{O}}}}}}
\newcommand{\jbmarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{J}}_{\textsc{B}}}}}}
\newcommand{\dbmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{D}}_{\textsc{B}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\nas}[1]{\arkcomment{\nasmarker}{#1}{red}}
\newcommand{\bo}[1]{\arkcomment{\bomarker}{#1}{blue}}
\newcommand{\jb}[1]{\arkcomment{\jbmarker}{#1}{orange}}
\newcommand{\db}[1]{\arkcomment{\dbmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\newcommand{\Sref}[1]{\S\ref{#1}}
\newcommand{\Snref}[1]{\S\ref{#1}:~\nameref{#1}}
\newcommand{\fref}[1]{figure~\ref{#1}}
\newcommand{\ffref}[2]{figures~\ref{#1} and~\ref{#2}}
\newcommand{\Fref}[1]{Figure~\ref{#1}}
\newcommand{\FFref}[2]{Figures~\ref{#1} and~\ref{#2}}
\newcommand{\tref}[1]{table~\ref{#1}}
\newcommand{\ttref}[2]{tables~\ref{#1} and~\ref{#2}}
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\aref}[1]{algorithm~\ref{#1}}
\newcommand{\Aref}[1]{Algorithm~\ref{#1}}
\newcommand{\appref}[1]{appendix~\ref{#1}}
\newcommand{\Appref}[1]{Appendix~\ref{#1}}
\newcommand{\fnref}[1]{footnote~\ref{#1}}
\newcommand{\eref}[1]{\eqref{#1}}

% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
\newcommand{\draftnotice}[1]{#1} % for the draft version only
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}

\title{\draftnotice{{\it\small WORKING DRAFT}\\[5pt] }Features in Models of Language:\\ From Parametrization to Implementation}

\author{Nathan Schneider}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\noindent Feature-rich models are ubiquitous in NLP. 
This note discusses concepts, terminology, algorithms, and implementation details pertaining to features in discrete statistical models.
\end{abstract}

We begin in {\bf \Snref{sec:prelim}} with a mathematical overview of the sorts of models 
that will be considered here.
New terminology (especially: \textbf{percept} and \textbf{cross-product parametrization})
and notation are explained.

The next two sections address {\em feature design}.
{\bf \Snref{sec:param}} elaborates on several mathematical issues of parametrization, 
such as the treatment of features over categorical inputs.
{\bf \Snref{sec:extraction}} offers an applied perspective on the development of feature functions---typically, 
this combines description of abstract templates with data-driven generation and selection of specific features.

Given a feature function, {\bf \Snref{sec:learning}} concerns {\em weight learning}, 
focusing on supervised training. {\bf \Snref{sec:interp}} discusses the insights that can be gained 
from examining learned model weights. 

To make the ideas in this document more concrete, 
a Python reference implementation of feature extraction and weight learning
appears in {\bf \appref{ex}}.

\section{Preliminaries}\label{sec:prelim}

We will consider feature-based models such as logistic regression 
that have discrete categorical outputs (classes, responses), which we will call {\bf labels}. 
The set of possible labels, denoted $\mathcal{Y}$, is assumed to be known and the same for all inputs.\footnote{In some cases, which we do not consider here, the space of possible outputs may depend on the input, so it is $\mathcal{Y}(x)$. 
For example, in structured problems like tagging and parsing, the space of possible predictions depends on the length of the input.}
Without loss of generality, we arbitrarily index the labels: $\mathcal{Y}=\{\mathrm{Y}^1,\mathrm{Y}^2,\ldots,\mathrm{Y}^{Y}\}$.

Given an input $x \in \mathcal{X}$, our decoding objective is 
to choose the label $y \in \mathcal{Y}$ that best fits the input. 
The function that does this is called a classifier: $h: \mathcal{X}\rightarrow\mathcal{Y}$.
We often operationalize $h(x)$ as maximizing a real-valued scoring function, $g_\params: \mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}$, 
where $\params$ represents the model parameters: 

\begin{equation}\label{eq:classifier}
\hat{y} = h(x) = \arg\max_{y\in\mathcal{Y}} g_\params(x,y)
\end{equation}

In NLP, $x$ is typically a linguistic unit---such as a word or document---with no inherent mathematical structure. 
A {\bf feature function} identifies characteristics of the input that are relevant to the mathematical model, 
and links them to the possible labels. 
For example, if $x$ is a document, then we may want to encode it as a bag of words 
in order to model similarity between documents as the number of words shared in common.

Our feature function, $\mathbf{f}$, is a vector-valued mapping between 
characteristics of the input and characteristics of the output.
We assume that $\mathbf{f}$ returns $d$ components---$\mathbf{f}: \mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}^d$.\footnote{The parameters in a model are sometimes called its {\bf weights}, 
though we reserve the term ``weight'' for the actual value of a parameter.}

The scoring function can then depend on the sum of the components of the vector returned by $\mathbf{f}$ weighted by corresponding parameters.
The (real) parameters weight the features to establish the goodness-of-fit between pairs of input-output characteristics.  
Here is a {\bf linear} scoring function, 
given parameters $\boldsymbol{\theta}\in\mathbb{R}^d$ such that $\theta_j$ is associated with the $j$th component of the feature vector:
\begin{equation}\label{eq:linear}
g_{\boldsymbol{\theta}}(x,y) = \sum_{j=1}^d \theta_j f_j(x,y)
= \boldsymbol{\theta}^{\top} \mathbf{f}(x,y)
\end{equation}
And here is a {\bf log-linear} scoring function, which composes the linear scoring function within the 
{\bf softmax} function (to scale it between 0 and 1), which is in turn composed within a logarithm:\footnote{The denominator (sum over all labels) is called the {\bf partition function}, sometimes denoted $Z_{\boldsymbol{\theta}}(x)$. It normalizes the scores to be a probability distribution (nonnegative and summing to 1).}
\begin{equation}\label{eq:softmax}
g_{\boldsymbol{\theta}}(x,y) = \log\frac{\exp{\sum_{j=1}^d \theta_j f_j(x,y)}}{\sum_{y'\in\mathcal{Y}}\exp{\sum_{j=1}^d \theta_j f_j(x,y')}}
= \boldsymbol{\theta}^{\top} \mathbf{f}(x,y) - \log{\sum_{y'\in\mathcal{Y}}\exp{\boldsymbol{\theta}^{\top} \mathbf{f}(x,y')}}
\end{equation}

Most commonly, the multiclass feature function looks like this:\nss{I am going for vector concatenation; there may be a better notation for this}
\begin{equation}\label{eq:crossprod}
\mathbf{f}(x,y=\mathrm{Y}^k) = \mathbf{0}_{c(k-1)};\  \boldsymbol{\phi}(x);\  \mathbf{0}_{c(Y-k)}
\end{equation}
where $\boldsymbol{\phi}: \mathcal{X}\rightarrow\mathbb{R}^c$ describes every input with $c$ real-valued\footnote{For discrete data, these are often binary, though the models we describe here do not depend on that. A document could be featurized with relative frequencies of its words, for example.} 
characteristics (the values of these characteristics are the same no matter what label is considered), 
and $d = |\boldsymbol{\theta}| = c \times Y$---there is one parameter for every pairing of an input characteristic with a label.\footnote{We assume 
for now that there is only one characteristic of each {\em output}, the identity of the label. \Sref{sec:richer} relaxes this assumption.}
In other words, $\left(\mathbf{f}(x,y=\mathrm{Y}^k)\right)_{c(k-1)+j} = \phi_j(x)$ for all $k\in\{1,2,\ldots,Y\}$.
We will refer to $\boldsymbol{\phi}(x)$ as the {\bf percept}\footnote{This is a nonstandard term; we are introducing 
it because the term ``feature'' is overloaded, sometimes referring to the raw characteristic of the input (what we mean by ``percept''),
and other times to the variable associated with a parameter, 
which depends on both the input and the output.\nss{manning \& schuetze p. 591 mentions this terminology issue}} function (a percept being a characteristic of the input),
and this parametrization of the feature function $\mathbf{f}$ as the {\bf cross-product}\footnote{The cross-product parameterization is the most common. 
In fact, many authors define multiclass logistic regression in terms of this parameterization; e.g., \citet{hastie-09}. 
These and other authors use the term ``feature'' to refer to what we call a ``percept.''}
parametrization, as there is one parameter for every pair of input-output characteristics.

For example: Suppose that our input $x$ is a document, and the label space $\mathcal{Y}$ consists of genre categories ({\sc article}, {\sc transcript}, {\sc recipe}, etc.).
In our terminology, one {\em percept} might be ``contains the word \textit{pie}''; the corresponding {\em features} 
would be ``{\sc article} contains the word \textit{pie}'', ``{\sc transcript} contains the word \textit{pie}'', ``{\sc recipe} contains the word \textit{pie}'', 
and so forth. When the {\sc article} label is being scored, only the features for the {\sc article} label are active,\footnote{i.e., have a nonzero value} 
and likewise for the other categories. With just these features, the model (assuming good parameters) will score 
a document with the word \textit{pie} as best matching the {\sc recipe} label, because the 
``{\sc recipe} contains the word \textit{pie}'' will have a higher weight than the other \textit{pie} features.
The model might include other percepts---other words, bigrams (``contains the bigram \textit{apple pie}''), 
metadata (``author = Martha Stewart''), or any other characteristics of the document deemed relevant to the task.
The cross-product parametrization implies that a feature is instantiated for every percept paired with every label.


Now we can define {\bf multinomial (or multiclass) logistic regression} model 
as a model over a fixed set of categorical outputs with the log-linear scoring function and the cross-product parametrization.
To simplify notation, we will let $\boldsymbol{\theta}^y$ denote the one of the $Y$ sections of $\boldsymbol{\theta}$ 
corresponding to the label $y$. 
%so $\left(\mathbf{g}_{\boldsymbol{\theta}}(x,y)\right)_j = \boldsymbol{\theta}^y_j \phi_j(x) = \boldsymbol{\theta}^{y\top} \boldsymbol{\phi}(x)$.
Then, combining equations \eref{eq:classifier}, \eref{eq:softmax}, and \eref{eq:crossprod}, 
the classification rule is
\begin{equation}\label{eq:logistic}
\hat{y} = \max_{y\in\mathcal{Y}} g_\params(x,y) = \max_{y\in\mathcal{Y}} \boldsymbol{\theta}^{y\top} \boldsymbol{\phi}(x) - \log{\sum_{y'\in\mathcal{Y}}{\exp{\boldsymbol{\theta}^{y'\top} \boldsymbol{\phi}(x)}}}
\end{equation} 
A {\bf log-linear}, or {\bf max(imum) ent(ropy)}, model is more general---it implies the log-linear scoring function 
but does not depend on the space of outputs being fixed or require the cross-product parametrization.

\nss{is it necessary to discuss generative/discriminative/joint/conditional?}
\\

\noindent To summarize: in our terminology,
\begin{itemize}
  \item {\bf Labels} are the classes/outputs/responses. In classification, one label $y$ is predicted for each input $x$; 
  we assume the set of possible labels is fixed in advance.
  \item A {\bf percept} is a characteristic of the input. The percept function $\boldsymbol{\phi}(x)$ maps an input to a vector of percepts.
  It is typically designed manually with knowledge of the task/domain.
  \item A {\bf feature} is a real-valued variable linking a percept to a label. The feature function $\mathbf{f}(x,y)$ maps an input and a label to a vector of features.
  \item A {\bf parameter} is a real-valued variable in one-to-one correspondence with a feature. Parameters $\boldsymbol{\theta}$ are typically learned from data.
  \item The {\bf parametrization} of a model is the correspondence between percepts and features/parameters. 
  Most commonly (and always in logistic regression), the {\bf cross-product} parametrization is used, in which every percept is linked to each label, one label per feature.
  \item The {\bf scoring function} $g_{\boldsymbol{\theta}}(x,y)$ assigns a real score to a given input and label. 
  The simplest is {\bf linear} (the dot product of the parameter vector with the feature vector).
  The {\bf log-linear} scoring function is useful for probabilistic models, and subsumes logistic regression.
  In classification, the highest-scoring label for the input wins. 
\end{itemize}

\section{Parametrization}\label{sec:param}

\subsection{Bias features}\label{sec:bias}

Because the overall frequency of the labels in data may not be uniform, 
it is customary to account for this base frequency with a {\bf bias feature} 
for each label. The bias feature fires (i.e., has value 1) whenever 
the label is being scored; the weight that is learned will be indicative of 
the label's frequency.\footnote{This is equivalent to an intercept term in the scoring function.} 
Under the cross-product parametrization, this amounts to a percept that is 1 for all inputs.
Bias parameters are sometimes exempted from regularization (see \Sref{sec:reg}).

\subsection{Background labels and degrees of freedom}\label{sec:bg}

Imagine a linear binary classifier whose only percept is a real value. 
This classifier can be represented with only one feature/parameter, a threshold\footnote{In higher dimensions, the geometric boundary between classes is called a {\bf separating hyperplane}.}
below which the input is assigned to one class and above which it is assigned to the other.
One {\em could} model this decision with two features/parameters, one that fires when scoring the positive class 
and one that fires for the negative class---but this is redundant because the negative-class feature value
is a function of the positive-class feature value (they are mutually exclusive).
The model is thus said to have one {\bf degree of freedom}.\footnote{\nss{how does this relate to {\bf overparametrization} and {\bf identifiability}?}}
More generally, with $Y$ mutually exclusive labels and percept functions in $\mathbb{R}^c$, a (log-)linear classifier 
never requires more than $(Y-1) \times c$ features. 
Implicitly, one of the labels---call this the {\bf background label}---can be assigned a score of 0 for all inputs, 
and the non-background labels are scored relative to 0 (a positive score for an input means it outranks the background label; 
a negative score means it is a worse fit). Designating the background label $\mathrm{Y}^{\textsc{bg}}$, the classifier can then be rewritten as

\begin{equation}\label{eq:classifier-bg}
\hat{y} = h(x) = \arg\max_{y\in\mathcal{Y}} \left\{\begin{array}{cl}
0 & \text{if }y=\mathrm{Y}^{\textsc{bg}} \\
g_\params(x,y) & \text{o.w.}\end{array}\right\}
\end{equation}

In practice, keeping parameters for all $Y$ labels (not making one of them a background label) 
should not affect predictive performance, and may make the model more interpretable (\Sref{sec:interp}).
But designating a background label gives savings in memory (smaller parameter vector) and runtime (one less class to score for each input).

\subsection{Categorical characteristics}

Models for discrete data commonly represent categorical characteristics of the input.
A binary characteristic is handled with a binary-valued percept; it is redundant to have 
nonzero parameters for both the ``false'' and ``true'' states, as these would be guaranteed 
to have opposite values if learned from data (because when one fires the other is silent, and vice versa).
In general, a characteristic with a closed set of $k$ possible categorical and mutually exclusive values 
is encoded nonredundantly with $k-1$ binary percepts.\footnote{A percept taking two or more nonzero values has a scalar 
interpretation under scoring functions such as \eref{eq:linear} and \eref{eq:softmax}. 
Hence the use of multiple binary percepts for categorical characteristics.}
Mathematically, there is no harm in representing all $k$ categories, 
and depending on the semantics of the characteristic, it might make the model more interpretable.
But when a characteristic has an obvious {\bf default} category (such as ``not applicable''),
it can safely be left out of the percept vector---much like features for a background label 
can be left out of the feature vector (\Sref{sec:bg})---with slight computational savings.

\subsection{Scalar characteristics}

\nss{binning and thresholding}



\subsection{Conjunctions and disjunctions}\label{sec:complex}

It is quite common to combine characteristics of the input to form a more specific percept.
For example, consider a sentiment classification task where the words of the input text have been 
tagged with a part of speech (POS). In addition to percepts for words (e.g., \textit{nice}) 
and for POS tags (e.g., \textsc{Adjective})---call these \textbf{atomic} percepts---these may be 
\textbf{conjoined} to form word-and-tag percepts (e.g., a percept that fires if and only if the input contains 
the word \textit{nice} tagged as an \textsc{Adjective}; or another that fires where the input contains 
the word \textit{nice} following an \textsc{Adverb}; and so forth).
These \textbf{complex} percepts can be more informative than the atomic percepts, 
perhaps because they are less ambiguous (\textit{nice} as adjective vs.~name of a city in France).
But conjoined percepts tend to be more sparse (less frequent in data), and thus their parameters might be harder to learn, 
so in a conditional model it is usually advisable to include both atomic and complex percepts, 
even if some are implied by others.

Another kind of complex percept that may be used is a \textbf{disjunction}: it is active if 
{\em any} of its disjuncts are active. For example, if a word is known to have spelling variants, 
like \textit{favor} and \textit{favour}, a feature that abstracts away from the spelling 
may make the model more robust to different input sources (say, American vs.~British).
Disjunctions can be interpreted as \textbf{parameter tying}---i.e., instead of separate parameters 
to be learned for \textit{favor} and \textit{favour}, these are ``tied'' together.
\nss{anything else to be said about parameter tying? do some people call it ``sharing''?}

Mathematically, complex percepts are treated just like atomic percepts, 
but the distinction can be exploited in implementation.
Feature induction (\Sref{sec:induction}) seeks to discover conjunctions or disjunctions of 
existing features/percepts that improve the discriminating power of the model.


\subsection{Richer features\nss{better name?}}\label{sec:richer}

Thus far, we have assumed that features link input characteristics (percepts) 
to a single label. But in general, a feature may be any real-valued function of the input and label.
Here are two examples:

\begin{itemize}
\item \textbf{Features making sophisticated use of labels.} 
When there is a known grouping or hierarchy of labels, a feature may combine a percept 
with a disjunction of related labels. This might help the model to learn better generalizations, 
especially if some labels are much rarer than others.

\item \textbf{Features making sophisticated use of the input-and-label combination.}
Consider a preposition correction model, where the input is a preposition in context 
and the labels are candidate replacement prepositions \citep[as in][]{han-10}. 
A feature template might encode the relative frequency of the label in the input context, 
as estimated from a large corpus (perhaps separate from the data on which the model is learned).
\end{itemize}

These are mathematically straightforward, but the sophisticated relationship between input and output 
is unusual and not anticipated by many implementations.

\subsection{Other topics?}

\nss{local vs.~nonlocal in structured models}

\nss{softmax trick?}

\section{Extraction}\label{sec:extraction}

This section describes how large feature functions are designed in practice
so as to harness the mathematical machinery described above.
Margin notes link to relevant portions of the reference implementation (\appref{ex}).

The process of encoding \marginpar{\small \hyperref[lst:56]{$\rightarrow$~\textsmaller{\texttt{extract()}}}}
each data point as percepts\slash features is called \textbf{extraction}.
As discussed below, the programmer often specifies abstract \textbf{templates} 
to be instantiated (filled in) in a data-driven fashion
using the same dataset on which parameter weights will be learned, 
i.e., the \textbf{training data}. (Weight learning is discussed in \Sref{sec:learning}.)
When handling features, a variety of implementation techniques 
are available to make effective use of runtime and memory resources.

\subsection{Templates}

NLP models routinely contain thousands or millions of features. 
When describing the model in code or prose, we do not actually list every single one of these features; 
instead, we employ abstractions known as \textbf{templates}. 
For instance, a document classification paper might say ``features are words in the document''---meaning, 
in our terminology, that each {\em percept} encodes a word type, and is active for any input document containing that word.
The percept template can be expressed with a variable: ``word $w$ appears in the document''; 
the template instantiates one percept for every word in some vocabulary 
(typically, the vocabulary of the training data). 
Complex templates contain multiple variables: we can generalize the first conjunction example given in the previous section to 
``the input contains the word $w$ tagged as with part of speech $p$.'' 
Universal quantification ($\forall w \in$ vocabulary, $\forall p \in$ POS tagset) 
is usually assumed, subject to limitations of the data.
Formally, a percept template $\tilde{\Phi}$ is a higher-order function 
that produces percept functions: $\forall w,p,\ \tilde{\Phi}(w, p) = \lambda w_x,p_x.\ \lsem w_x=w \wedge p_x=p \rsem$.

{\bf Open-ended templates} contain some variable whose possible values are data-dependent, not predetermined. 
Natural language words/strings are a good example.
By contrast, {\bf closed templates} contain only variables with predefined sets of possible values, 
so all instantiations of a closed template can be enumerated ahead of time. 

For a given instance and template, let \textbf{cardinality} refer to the number of active instantiations of the template.
A template's cardinality is either \textbf{fixed} across instances, or is \textbf{flexible}, i.e., it may vary from instance to instance.
E.g., consider the closed template ``input contains at least $k$ words'', where $k$ ranges from 100 to 1000 in increments of 100.
Because this template performs thresholding, anywhere from 0 to 10 percepts may be active depending on the length of the input.
But for the binning template ``input contains around $k$ words (rounded to the nearest 100)'', only one percept 
will be active per input. Similarly, an open-ended template may have a known or variable number of instantiations per input: 
``the $k$th word in the input is $w$'' ($k$ ranging from 1 to 5) has fixed cardinality, whereas 
``the input contains word $w$'' has flexible cardinality.

Sometimes the template description includes a variable for the label, 
in which case it is a feature (rather than percept) template.
Indeed, a regular mapping from percepts to features, 
as in the cross-product parametrization, can be described in terms of a feature template 
whose variables are percepts and labels: 
%``percept $\phi$ and label $y$ $\forall \phi\in\Phi, y\in\mathcal{Y}$.''
$\forall \phi\in\Phi, y\in\mathcal{Y},\ \tilde{F}(\phi,y) = \lambda x,y'.\ \phi(x) \cdot \lsem y'=y \rsem$.

\nss{feature template metalanguages}

Feature extraction code often handles both nontemplatic percepts (e.g., ``input starts with a capital letter'') 
and templatic percepts (e.g., ``input starts with the character $k$'') together.

\subsection{Instantiation and recognition}\label{sec:inst}

Feature extraction code has to be used for two tasks:
\begin{itemize}
  \item \textbf{Instantiation:} \marginpar{\small \hyperref[lst:81]{$\rightarrow$~\textsmaller{\texttt{instantiate()}}}}
  Decide which percepts\slash features will be included in the feature function used by the model. 
  If there are templates (particularly open-ended ones), an initial pass is usually made 
  to determine which instantiations of the templates are needed for the training data.
  \item \textbf{Recognition:} \marginpar{\small \hyperref[lst:129]{$\rightarrow$~\textit{line 129}}}
  Given the model's feature function, enumerate the active percepts\slash features and their values.
  This is performed for each instance during training\footnote{To avoid repeating extraction on the training data 
during each iteration of learning, consider caching each instance's percepts\slash features (see \nss{TODO}).} and at test time.
\end{itemize}

%\paragraph{Unsupported percepts and features.}
Many percepts and features, especially instantiations of open-ended templates, will never be active in the training data. 
These are said to be \textbf{unsupported}.
Note that no unsupported {\em percept} will receive nonzero parameter weights during learning, 
so only percepts attested in the training data (found during instantiation) 
need to be included in the feature function (and accessed during recognition).\footnote{In the framework of (log-)linear 
models addressed here, the only exception can be if a special regularizer is used that might push weights for unsupported percepts away from 0. 
See \nss{TODO}.}
To put this in perspective, instantiating a word template for every vocabulary item in the English language would be vastly ineffecient!
On the other hand, unsupported {\em features} for supported percepts---percept-label pairs never seen in the training data---may be retained, 
and can be slightly beneficial to the model by helping steer the classifier away from incorrect predictions. 

%\paragraph{Cutoffs.}
Sometimes a \textbf{cutoff} is used to remove infrequent percepts\slash features as measured by the training data.
Active percepts\slash features are counted during instantiation, and then any occurring fewer than $k$ times 
are discarded from the feature function. (The usual no-unsupported-percepts rule amounts to $k=1$ for percepts, 
though this can be made stricter; a no-unsupported-features policy would apply a $k\geq 1$ cutoff to individual features.)
For sufficiently small (relative to the size of the training data) values of $k$, 
this will have little impact on accuracy but will make learning and classification more efficient. 

\subsection{Implementing extraction and lookup}

Extracting/accessing a large number of features can be a major performance bottleneck. 
We discuss the efficiency tradeoffs of several feature handling optimizations.

\subsubsection{Optimizations for both memory and runtime}

\begin{itemize}
\item \textbf{Sparse vs.~dense data structures.} 
\marginpar{\small \hyperref[lst:57]{$\rightarrow$~\textsmaller{\texttt{active\_percepts}}}}
The number of active percepts\slash features for an instance 
is generally much smaller than the number of parameters \nss{refer to sparsity discussion above}, 
so a sparse data structure such as a map should be used rather than a length-$d$ vector.
\marginpar{\small \hyperref[lst:102]{$\rightarrow$~\textsmaller{\texttt{weights}}}} 
On the other hand, a dense vector/array data structure should be used for the parameter weights
if most weights will be nonzero (that is, unless the model enforces sparsity).

\item \textbf{Feature indexing.}
Features should be indexed to integers so weights can be stored in an array for fast dot product computation.
\begin{itemize}
\item \textbf{Implicit feature indexing.} \marginpar{\small \hyperref[lst:66]{$\rightarrow$~\textsmaller{\texttt{feature\_index()}}}}
With the cross-product parametrization \eref{eq:crossprod}, 
feature indices should be computed on the fly.
First number the labels, 
and map percept names to integers (by storing an explicit mapping to unique indices, or by hashing).
The weight vector can then be organized such that the feature index 
is a simple function of the percept and label indices.
\end{itemize}

\item Choose a \textbf{background label} (\Sref{sec:bg}), 
\marginpar{\small \hyperref[lst:74]{$\rightarrow$~\textsmaller{\texttt{labels[-1]}}}}
and\slash or apply a percept or feature \textbf{cutoff} 
\marginpar{\small \hyperref[lst:34]{$\rightarrow$~\textsmaller{\texttt{percept\_cutoff}}}}
such as removing unsupported features (\Sref{sec:inst}), 
in order to limit the dimensionality of the feature function.
It is worth experimenting with a few alternatives, because cutoffs---especially 
in combination with a background label\footnote{Consider a percept $\phi$ observed frequently in the training
data, always with label $y$. Intuitively, this percept should be a good predictor of the label $y$---but
if $y$ is designated as the background label and no unsupported features are included,
$\phi$ will not be represented at all in the model!}---may affect the model's predictive power.
\end{itemize}

\subsubsection{Optimizations for memory}

\begin{itemize}
\item \textbf{String vocabulary compression.}
Na\"{i}ve handling of strings may result in many duplicate copies in memory 
(e.g., of any word that appears in multiple instances). 
There are a few techniques to avoid this when loading instances or during extraction:
\begin{itemize}
\item \textbf{Interning.} This operation checks to see whether an identical string has been previously interned, 
and if so returns that copy to eliminate duplication.
Interning functionality is usually built in to the programming language (e.g., via an \textsmaller{\texttt{intern()}} method on strings), 
though a map can be used instead to store canonical copies of strings.
% mention that IdentityHashMap is available if all strings are interned?
\item \textbf{Explicit indexing.} \marginpar{\small \hyperref[lst:41]{$\rightarrow$~\textsmaller{\texttt{index\_string()}}}}
Assign a unique integer to each string that is read or produced 
and use the integers (rather than strings) in the instance's features. 
This is the most straightforward approach to indexing.
A separate mapping between strings and integers will have to be maintained. 
\item \textbf{Hashing.} Strings can instead be hashed to obtain an index, 
though the index is not guaranteed to be unique.
\end{itemize}

\item \textbf{Bundling.} \marginpar{\small \hyperref[lst:45]{$\rightarrow$~\textit{line 45}}}
Storing each complex percept in a lightweight tuple-like data structure rather than as a concatenated string 
allows atomic parts of the percept to be reused. 
(This may also improve runtime if string concatenation is expensive.)
\end{itemize}

\subsubsection{Optimizations for extraction runtime}

\begin{itemize}
  \item \textbf{Indexing data structures.} For fast lookup with explicit indexing of strings or tuples, 
use a trie or a map with an efficient hash function.

  \item \textbf{Percept/feature hashing.} An extreme form of percept or feature indexing is to use a hash function to compute the index 
(without having to consult a data structure).\footnote{See \citep{weinberger-09} and \url{http://en.wikipedia.org/wiki/Feature_hashing}.}
With a fast hash function, this reduces runtime overhead, 
and if the names of the percepts\slash features are discarded following hashing, it saves memory as well.
The downside is that hash values are not guaranteed to be unique; 
thus, there is a chance that multiple percepts\slash features will 
be mapped to the same parameter, though occasional collisions should have minimal impact in practice. 

  \item \nss{parallelization?}
\end{itemize}

\subsubsection{Optimizations for scoring runtime}

\begin{itemize}
\item \textbf{With only binary features.} If all percepts\slash features in the model are binary, 
then the dot product score is simply the sum of weights of active features: no multiplication is required.
\end{itemize}

% - alternative suggested by Sam: use locality-sensitive hashing to project to a low-dimensional binary feature space. 
%   then, instead of a dot product, compute a Hamming distance. 
%   in learning, the training instances are projected once, and the weight vector is re-projected each time 
%   it is updated (so probably not ideal for online learning algorithms). 
%   see \citep{?} for details.


\section{Learning}\label{sec:learning}

\nss{estimation: objective, optimization---practical issues (e.g., fast score computation, sparse updates/averaging)}
% (kinds of optimization: online vs. (mini)batch; gradient-based; margin-based vs. probabilistic; curriculum; joint/constrained/dual decomp)

\nss{unsupported features/count thresholds; OOV handling}

\subsection{Feature induction}\label{sec:induction}

\subsection{Regularization}\label{sec:reg}

\nss{regularization: simple approaches, sparsity, groups, graph-based, priors}

\nss{pointers to un/semi-supervised approaches}
% - semi-supervised learning/domain adaptation: posterior regularization; active learning; transfer learning
% - unsupervised learning: generative better?; Berg-Kirkpatrick

\subsection{Implementing learning}

\nss{implementation issues: store features in memory/disk?; sparse updates}
\subsubsection{Optimizations for learning}
% runtime savings during learning
% - cache the extracted percepts for each instance so they do not have to be recomputed on each iteration


\section{Interpretation}\label{sec:interp}

\nss{caveats about interpretability}

\nss{impact measure}


\appendix
\section{Reference implementation}\label{ex}
%caption={Example classifier implementation in Python.},
\lstinputlisting{classifier.py}

\bibliographystyle{plainnat}
\bibliography{features}



\end{document}
